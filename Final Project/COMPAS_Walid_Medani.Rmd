---
title: <br><center>A Diary Study on Algorithmic Auditing:Analyzing Racial Bias in the Algorithm of COMPAS</center>
author: "<center> Walid Medani </center>"
date: "<center> 05/04/21 </center>"
output:
  html_document: default
  pdf_document: default
indent: TRUE
mainfont: Times New Roman
fontsize: 12pt
bibliography: "compascitations.bib"
header-includes:
    - \usepackage{setspace}\doublespacing
---

```{=html}
<style>
p {line-height: 2em;}
p {font-size: 1.4em;}
p {font-family: Times New Roman;}

blockquote {
padding: 40px;
font-family: Times New Roman;
font-size: 1em;
border: none !important;
}
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(kableExtra)
library(ggplot2)
library(tufte)
library(ggthemes)
library(viridis)
library(tinytex)
library(caret)
library(fairness)
library(bibtex)
library(latexpdf)
library(caret)
library(naniar)
```

<br> As the overton window in public opinion shifts to more accountability, transparency and oversight in algorithms or black box models. The ethical dilemma of using models as the sole or major factor in deciding incarceration has permeated the discourse surrounding Risk Assessment Instruments (RAI). RAIs are algorithmic tools that assess a defendant's future risk of recidivism and court appearance based on a variety of factors and historical group tendencies. However, crime is an artificial concept and the way it's defined is subjective. The methods to measure it are imperfect and often display racial biases. It discounts and reinforces structural racial disparities in environmental factors, material conditions, education, and population density. In a 2013 study, [@olver2014] found that ethnic minorities had higher risk scores in the U.S. in comparison to Canada and noted that "systemic bias within the justice system may distort recidivism". RAIs' predictive accuracy is also not supported by current evidence [@douglas2017],

> Existing data suggest that most risk assessment tools have poor to moderate accuracy in most applications. Typically, more than half of individuals judged by tools as high risk are incorrectly classified---they will not go on to offend [13]. These persons may be detained unnecessarily. False positives may be especially common in minority ethnic groups [14], [15].

The trade-secrets of these RAIs is currently justified under our current set of intellectual property laws, but it's unethical to justify private profiteering when it impacts incarceration. As algorithms models decide upon the balance of positive and negative impacts, the public requires transparency to understand how persons are being sorted and judged in order to have freedom from bias. The need for transparency is imperative due to the validity of RAIs being examined mostly by the same people who developed the instruments [@desmarais2013].

In 2016, ProPublica questioned the validity of the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) in predicting recidivism and racial bias. COMPAS is an RAI that is widely used throughout the U.S. in pretrial and sentencing. It analyzes a questionnaire of 137 variables such as age, sex, criminal history and computes a scale for recidivism at the time of booking in jail. COMPAS does not use race in factoring its recidivist scores. To ensure equitable application, ProPublica wanted to determine if these risk scores were in fact accurate and void of racial biases despite it not accounting for race. To examine the collective capacity of auditing algorithms, with only knowledge of introductory statistics and programming in R, I will attempt to replicate ProPublica's findings to journal my experiences and the cognitive labor necessary for algorithmic literacy. <br>

# Data & Visualization

To analyze bias within COMPAS, I will be using the [data](https://github.com/propublica/compas-analysis) ProPublica collected of 18,610 criminal defendants in Broward County. Broward County was specifically chosen since it utilizes COMPAS to oversee a large population size and benefits from Florida's transparent public record laws. Scores of criminal defendants that were assessed in stages outside of pre-trial were discarded, reducing the total criminal defendants down to 11,957 people.

```{r echo=FALSE, message=FALSE, warning=FALSE}
violentraw <- read.csv("./compas-scores-two-years-violent.csv")
compas_raw <- read.csv("./compas-scores-two-years.csv")
head(compas_raw) %>% 
kbl() %>%
kable_material(c("striped", "hover"), full_width = FALSE) %>% 
scroll_box(width = "100%", height = "auto")
```

<br> In order to clean up the raw data, defendants that were not screened by COMPAS within 30 days of the arrest were dismissed to ensure quality. Those who did not receive a COMPAS screening or committed traffic offenses with no jail time were removed.

```{r echo=FALSE}
compasnon <- compas_raw %>%
        select(age, age_cat, c_charge_degree, race, score_text, sex, priors_count, days_b_screening_arrest, decile_score, is_recid, two_year_recid, c_jail_in, c_jail_out) %>%
        mutate(is_med_or_high_risk = as.numeric(decile_score>=5, labels = c(0,1))) %>%
        filter(days_b_screening_arrest <= 30) %>%
        filter(days_b_screening_arrest >= -30) %>%
        filter(is_recid != -1) %>%
        filter(c_charge_degree != "O") %>%
        filter(score_text != 'N/A') %>% 
        mutate(length_of_stay = as.numeric(as.Date(c_jail_out) - as.Date(c_jail_in)))

compasviolent <- violentraw %>% 
select(age, age_cat, c_charge_degree, race, v_score_text, sex, priors_count, 
                    days_b_screening_arrest, v_decile_score, is_recid, two_year_recid) %>%
        mutate(is_med_or_high_risk = as.integer(v_decile_score>=5)) %>% 
        filter(days_b_screening_arrest <= 30) %>%
        filter(days_b_screening_arrest >= -30) %>% 
        filter(is_recid != -1) %>%
        filter(c_charge_degree != "O") %>%
        filter(v_score_text != 'N/A')

head(compasnon) %>% 
kbl() %>%
kable_material(c("striped", "hover")) %>% 
scroll_box(width = "100%", height = "auto")
```
Before and after of cleaning missing values in the dataset:
```{r missing, echo=FALSE, fig.show="hold", out.width="50%"}
vis_miss(compas_raw)
vis_miss(compasnon)
```

### <br>Below is the demographic breakdown of the sample of non-violent and violent criminal defendants processed by COMPAS in Broward County.

```{r dist, echo=FALSE, class.source="chunkcolor", fig.show="hold", out.width="50%"}
compasnon %>% 
ggplot(aes(x = race, fill = sex))+
  geom_bar(position = 'dodge') +
    labs(title = "Ethnic & Gender Distribution of 
Non-Violent Criminal Defendants",
       subtitle = "Broward County") +
    theme_fivethirtyeight() +
    scale_fill_viridis_d() 

compasviolent %>% 
ggplot(aes(x = race, fill = sex))+
  geom_bar(position = 'dodge') +
    labs(title = "Ethnic & Gender Distribution of 
Violent Criminal Defendants",
       subtitle = "Broward County") +
    theme_fivethirtyeight() +
    scale_fill_viridis_d() 
# scale_fill_manual(values = c("#440154FF", "#5DC863FF"))
```

```{r age, echo=FALSE, class.source="chunkcolor", fig.show="hold", out.width="50%"}
compasnon %>% 
ggplot(aes(x = age, fill = sex))+
  geom_bar(position = 'dodge') +
    labs(title = "Age & Gender Distribution of 
Non-Violent Criminal Defendants",
       subtitle = "Broward County") +
    theme_fivethirtyeight() +
    scale_fill_viridis_d() 

compasviolent %>% 
ggplot(aes(x = age, fill = sex))+
  geom_bar(position = 'dodge') +
    labs(title = "Age & Gender Distribution of 
Violent Criminal Defendants",
       subtitle = "Broward County") +
    theme_fivethirtyeight() +
    scale_fill_viridis_d() 
```

COMPAS's decile scores are rankings of normative groups in ascending order. In [Northpointe's practitioner guide](http://www.northpointeinc.com/downloads/compas/Practitioners-Guide-COMPAS-Core-_031915.pdf) for COMPAS, decile scores are interpreted as following:

-   **1 -- 4**: scale score is **low** relative to other offenders in norm group.

-   **5 -- 7**: scale score is **medium** relative to other offenders in norm group.

-   **8 -- 10**: scale score is **high** relative to other offenders in norm group.

We start to visually see bias in the data as shown in the figures below. The trend for Caucasian decile scores decreases as the normative scale score increases, whereas for African-Americans it is much more distributed throughout the rankings. Although scores for Native Americans is high, we will later on find out it's not statistically significant due to a sample size of 11 criminal defendants. \linebreak

```{r boxplot, echo=FALSE, fig.show="hold", out.width="50%"}
compasnon %>% 
  ggplot(aes(race, decile_score, fill = race)) +
  geom_boxplot(alpha = 0.8) +
  coord_flip() +
  theme_fivethirtyeight() +
  labs(title = "Dispersion of Non-Violent Decile Scores") +
  scale_fill_manual(values = c("#482677FF", "#404788FF", "#FDE725FF", "#238A8DFF", "#5DC863FF", "#73D055FF")) +
  theme(legend.position = "none")

compasviolent %>% 
  ggplot(aes(race, v_decile_score, fill = race)) +
  geom_boxplot(alpha = 0.8) +
  coord_flip() +
  theme_fivethirtyeight() +
  labs(title = "Dispersion of Violent Decile Scores") +
  scale_fill_manual(values = c("#482677FF", "#404788FF", "#FDE725FF", "#238A8DFF", "#5DC863FF", "#73D055FF")) +
  theme(legend.position = "none")
```

```{r decilegraphics, echo=FALSE, fig.show="hold", class.source="chunkcolor", out.width="50%"}
compasnon %>%
  filter(race == "African-American") %>% 
  ggplot(aes(ordered(decile_score))) +
  geom_bar(fill="#440154FF") +
  ylim(0, 650) +
  labs(title = "African-American Non-Violent Defendant Decile Scores")+
  theme_fivethirtyeight()

compasnon %>%
  filter(race == "Caucasian") %>% 
  ggplot(aes(ordered(decile_score))) +
  geom_bar(fill="#FDE725FF") +
  ylim(0, 650) +
  labs(title = "Caucasian Non-Violent Defendant Decile Scores")+
  theme_fivethirtyeight()
```

```{r vdecilegraphics, echo=FALSE, fig.show="hold", class.source="chunkcolor", out.width="50%"}
compasviolent %>%
  filter(race == "African-American") %>% 
  ggplot(aes(ordered(v_decile_score))) +
  geom_bar(fill="#440154FF") +
  ylim(0, 700) +
  labs(title = "African-American Violent Defendant Decile Scores")+
  theme_fivethirtyeight()

compasviolent %>%
  filter(race == "Caucasian") %>% 
  ggplot(aes(ordered(v_decile_score))) +
  geom_bar(fill="#FDE725FF") +
  ylim(0, 700) +
  labs(title = "Caucasian Violent Defendant Decile Scores")+
  theme_fivethirtyeight()
```

Those who receive a decile score of 5 or more are classified as medium to high risk in relation to the normative group. By taking the mean of those who were predicted to recidivate (decile scores 5-10) and the actual recidivism rate, we can find the accuracy of recidivism predictions. We find that COMPAS is 62% accurate in predicting non-violent recidivism and 35% accurate for predicting violent criminal defendants.

```{r accuracy, echo=FALSE}
nonviolent_accuracy <- mean(compasnon$is_med_or_high_risk | compasnon$two_year_recid)
violent_accuracy <- mean(compasviolent$is_med_or_high_risk | compasviolent$two_year_recid)

tibble(nonviolent_accuracy, violent_accuracy)*100
```

Since we have data on those who did recidivate within two years of receiving a decile score. We can run a confusion matrix to evaluate the performance of COMPAS's classification model in predicting recividism.

-   True positives (TP): predicted to recidivate and did recidivate (correct classification)

-   False positives (FP): predicted to recidivate but did NOT recidivate (incorrect classification)

-   True negatives (TN): NOT predicted to recidivate and did NOT recidivate (correct classification)

-   False negatives (FN): NOT predicted to recidivate but did recidivate (incorrect classification)

By using the formula ^FP/TP+FP^ we can find the false positive rate (incorrect classification). African American defendants had a 35% false positive rate in comparison to 28% in Caucasians.

```{r matrix, echo=FALSE, fig.show="hold", out.width="50%"}
Afcompas <- compasnon %>% 
  filter(race == "African-American")

ccompas <- compasnon %>% 
  filter(race == "Caucasian")

confusionMatrix(data = factor(Afcompas$is_med_or_high_risk), reference= factor(Afcompas$two_year_recid), positive = "1") 
confusionMatrix(data = factor(ccompas$is_med_or_high_risk), reference= factor(ccompas$two_year_recid),
positive = "1") 
ctable <- as.table(matrix(c(873, 473, 641 , 1188), nrow = 2, byrow = TRUE))
fourfoldplot(ctable, color = c("#440154FF", "#FDE725FF"),
             conf.level = 0, margin = 1, main = "African American Confusion Matrix")

ctable2 <- as.table(matrix(c(999 , 408, 282, 414), nrow = 2, byrow = TRUE))
fourfoldplot(ctable2, color = c("#440154FF", "#FDE725FF"),
             conf.level = 0, margin = 1, main = "Caucasian Confusion Matrix")
```

The distributions of decile scores might visually indicate bias but it does not take into consideration other factors that may be impacting it. Logistic regression is the appropriate regression analysis to conduct when the dependent variable is binary and we want to explain the relationship between one dependent binary variable and one or more independent variables. To test racial bias in decile scores, the logistic regression model will control for race, age, criminal history, future recidivism, charge degree, gender and age.

```{r glm, echo=FALSE}
compasglm <- compasnon %>%
    mutate(crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race)) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(score_text != "Low", labels = c("LowScore","HighScore")))

logistic<- glm(data=compasglm, is_med_or_high_risk ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + two_year_recid, family="binomial")
summary(logistic)
```

```{r glmv, echo=FALSE}
compasglm_v <- compasviolent %>% 
  mutate(crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race,
                                  labels = c("African-American", 
                                             "Asian",
                                             "Caucasian", 
                                             "Hispanic", 
                                             "Native American",
                                             "Other"))) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(v_score_text != "Low", labels = c("LowScore","HighScore")))

logisticv <- glm(data=compasglm_v, score_factor ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + two_year_recid, family="binomial")
summary(logisticv)
```

Based on the coefficients above, African American defendants were 47% more likely to receive a medium or high score. African American violent defendants were 65% more likely to receive a medium or high score. However these numbers aren't necessarily perfectly accurate due to the need of further calculating predicted probabilities.

\pagebreak

# Reflections

As a beginning student in the field of data science, the process of analyzing bias in datasets was a laborious task. Data preparation is a time-consuming but critical component in preparing data for exploratory analysis. But the ways to manipulate data are endless, and without a thorough background in statistics, it is easy to poison the data generating your analysis. Luckily there were a few case studies on COMPAS for me to learn from on how to manipulate datasets in order to begin my investigation. Once the data was cleaned, I was able to perform exploratory data analysis to summarize the characteristics of the dataset and formulate hypotheses without any modeling. This is the statistical method a beginner can perform to measure or visualize bias in data through discovering patterns, spotting anomalies, and checking assumptions with summary statistics.

The next step after conducting exploratory data analysis however is complex and requires the need of creating models and algorithms. I aspired to model the predictive accuracy of COMPAS's decile scores. I spent hours trying to learn the basics of machine learning and how to build classifiers to train my dataset. But ultimately I didn't have the foundation in multivariate statistics to apply models such as Cox Proportional Hazards or Survival Analysis to test for collider bias or casual influence. Open-source models to measure algorithmic fairness were readily available and comprehensively documented (e.g. [IBM's Fairness 360](https://aif360.mybluemix.net/), [Fairness in R](https://github.com/kozodoi/fairness)), with metrics such as demographic parity, proportional parity, ROC AUC comparison, etc. Yet it was still difficult for a beginner to implement these machine learning metrics without being cognizant of sensitive variables and which techniques to utilize in order to supplement these metrics. As [@kozodoi2020] points out in relation to applying fairness metrics in COMPAS,

> First, excluding ethnicity from the features slightly increases precision for some defendants (Caucasian and African_American) but results in a lower precision for some other groups (Asian and Hispanic). This illustrates that improving a model for one group may cost a fall in the predictive performance for the general population. Depending on the context, it is a task of a decision-maker to decide what is best.

Executing the models mentioned above with statistical computing was fairly easy, however interpreting the output from these models obstructed my exploration. This form of algorithmic opacity results from the fact that writing and reading code is a specialized activity [@burrell2016machine]. Even with publicly available open source models, the operation remains largely incomprehensible for those without specialized training. <br>

# Conclusion

ProPublica's analysis of COMPAS found a false positive rate (defendant is predicted as medium/high risk but does not re-offend) for African-American criminal defendants much higher than it is for caucasian ones, specifically for violent crimes. These findings sparked a debate between academics and RAI developers in what constitutes fairness. COMPAS's defense is that the proportion of recidivism is the same regardless of race, therefore making the algorithm fair. We would consider it unfair If the algorithm was to assign caucasians higher decile scores in order to mitigate racial bias.

However the assumption underlying this debate is that we blindly accept and rely on digital technology to accomplish ordinary goals. In a study to determine the need of such software, [@dressel] recruited 400 non-expert participants from Amazon's Mechanical Turk to predict recidivism with only seven variables in comparison to COMPAS's 137; participants had a prediction accuracy of 63 percent in comparison to the 67 percent of COMPAS. In another study, [@angelino2017] developed an algorithmic model comprising of only a criminal defendant's sex, age, and prior convictions to replicate the same predictive accuracy as COMPAS. The appeal of algorithms and big data may influence a judge's decision but would they still consider a RAI to set bail and sentencing if it performs the same as random surveyors? The problem is this reliance on algorithmic systems as oracles and without proper interpretation, the decision-making of algorithmic systems could devolve to perceive meaningful connections between seemingly unrelated things. And when these patterns are made, it could erode the dignity and autonomy of people and impose on their freedom from bias.

What ProPublica's analysis points to is most concerning possible sources of bias, which can come from the historical outcomes that an RAI learns to predict. Due to systemic racism engulfing the criminal justice system, models will only learn to replicate the outcomes of unjust practices. African Americans have historically been convicted at higher rates and racial disparity is exhibited in many forms from wage gaps, over-policing, education, sentencing, parole, and to bail. Datasets do not address the underlying social and structural hierarchies at play and are only modeled around the status quo. In order to circumvent racism in technology, [@benjamin2019] profoundly pinpoints the concept of how "blackness can be both marginal and focal to tech development". That if we compare and contrast ostensibly different technologies we can better sort through what is consequential to racial inequality.

\pagebreak

# Bibliography
